## Response to Automating Inequality

* **How do technical tools promise to "fair out" the remaining discrimination that exist in social/welfare systems? In how far can they succeed, in which ways do they fail?**

I remember when I first started to learn programing, there is a quote in the video saying that,  “The computer is good at doing repetitive task. And it’s the duty of people who write the code and  design the algorithm to think about how to give good instructions.” (The author also talked something about ‘design’ in that broadcast.) Here when talking about how do the technical tools promise to ‘fair out’ the remaining discrimination in the current social/welfare system, there are some concepts needed to be clarified before going to the judgment. How should we decide the baseline for ‘success’, and what can be considered as fairing out discrimination? The same task can bring us to the conclusion of both ‘succeed’ and ‘fail’. For example, in the example of Indiana given by Virginia Eubanks, the automatic system barely consider the context and considers any missing interviews as intentional and blocks many people. This is similar to the example of working visa Leon mentioned during the class. This failure is due to the lack of ‘humanity’ for these algorithms. But on the other hand, this kind of rating algorithm is widely used in our daily life. And in some cases it is also considered as succeed because they are more efficient when dealing with bunch of data and treat people with the same standard and fair out discrimination in a sense.

* **Imagine, what could this (following quotes) mean in the widest sense?"The state doesn't need a cop to kill a person" and "electronic incarceration”**

Virginia mentioned this quote when talking about the worse case it could happen due to the automating inequality. And in both her examples and in the widest sense, it has something to do with the social control. The ‘electronic incarceration’ refers to the situation when people’s rights are harmed by the decisions made by the computer such as the denial to access certain social welfare services (for example, the woman with cancer). And in these cases it’s even more difficult for people to get compensated because the computer is used as the excuse. 

* **What do you understand this to mean?"systems act as a kind of ‘empathy-overwrite'"**

The ‘empathy-overwrite’ is based on the assumption that there is no enough resources to help every people therefore the system should make sure people who mostly in need will get the help. This ranking task to handed over to the algorithm which is considered objective. But in reality is that the computer which seems to oversimplify the situation and ignore the ‘context’ is not capable of giving fair judgment. It roughly ranks people based on certain rules. But suffered people who are ruled out from the system means much more than the overwritten data.  

* **China is much more advanced and expansive when it comes to applying technical solutions to societal processes or instant challenges (recent example). Try to point example cases in China that are in accordance or in opposition to the problematics discussed in the podcast. Perhaps you can think of"technical systems not well thought-through about what their impact on human beings is" **

From my personal experience, the awareness of privacy data for people in China seems to be less strong than people in the US. The reports on NY times cover Chinese government’s monitor measures for several times, from facial recognition to this recent coronavirus example. While there are concerns about hidden privacy leaking, there are also news report on Chinese social media about how criminals got caught during this epidemic because “they have no where to hide under the strict monitor of the government”. In terms of building system of citizen’s data, I think Chinese government is taking a lead. And that’s always a double-edged sword. And it’s not only about whether there is measure or not, but also how far they go.
 
