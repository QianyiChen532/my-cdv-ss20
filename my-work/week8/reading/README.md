## "Radio Lab Right to be forgotten"

Both podcasts mention how can our “digital footprint” (whether it’s made by ourselves or others) affect our daily life and on the system level, what’s the interaction between individuals and the systems around the right of using these data. It reminds me of the discussion we had at the beginning of this semester about the usage of public published data. I personally always think it’s very difficult to deal with public records and data. It’s relatively easy to make the judgment when the issue is very extreme, like most of the people will agree to keep the record of a murder or someone with a very serious crime. But when it comes to the middle ground many things become arguable. 

I think this question can be broken down into two main parts, one is the data storage and one is the access. Deleting data from the database is an irreversible operation, but deleting or restricting the access to the data sounds more reasonable to me. Especially for crime and credit records, limiting the public access to it might help to prevent the widespread of data but also allowing other institutions(like the company or other stakeholders) to be informed.

It also reminds of me of an episode in black mirror describing a society where everyone has a score which affects their social status and resources. The creepy part in this setting is that the past record is cumulative, and it’s easy for the people to fall into a loop: bad behavior(or mistake) - bad social status and bias from others - lower self-confidence or worse psychological status - bad behavior(or mistake), and vice versa. If we’re going to have some guidelines on using public records and data, we should also take this kind of chain reaction into consideration.

## "The Daily The End of Privacy as we know it?"

I think systems like ClearView are often discussed under the topic of facial recognition and data privacy. And it’s clear that it’s a double-edged sword. If the government is the user of this system, the benefit will be to help police track the crime, enforce the law and the risks of using the system is the abuse of the surveillance power and further implement a strict monitor network that harms the privacy of citizens. If the user becomes a company or individuals, the issue becomes more complicated. Some people might use it for fun like I’ve seen the concept of APPs like ‘finding another yourself in the world’. More in more general cases, handing this kind of tool to the public will be like open the pandora’s box.

In this video, a journalist from BBC tests the surveillance system in Guiyang, China and shows how the police find people using the system [within 7 minutes](https://www.bbc.com/news/av/world-asia-china-42248056/in-your-face-china-s-all-seeing-state). Personally I partially agree with the analogy of weapon, saying this kind of technology is like weapons, the tool itself is neutral. It is the user and the usage of it that decide whether the issue is good or bad. But in reality, it’s never that simple and the blurry boundary is always making things more complicated.

In terms of regulation, it also depends on how the system is used. For example, since the result is not always accurate, (e.g. [Facial recognition tech misidentified 26 California lawmakers as criminals](https://www.engadget.com/2019-08-14-aclu-facial-recognition-police-body-cameras.html)) and that can lead to decisions of holding back this kind of system before it is ready.
