## Reading Response

### Examples:

- The data of customers’ profile. Many marketing teams are using this kind of data to categorize the customers and predict the result of some social media campaigns, especially when the budget is limited.
- The data of the temperature in the past few years are often used to predict the trend of global warming. The analysts also use different increase rate to predict the result under different circumstances.
- Career prediction. Students take survey about their interest and personality and are matched to the career in certain industries.

### Reflection:

In area like marketing, the prediction based on big data could be accurate and insightful since big data helps to illustrate what the target customers have in common. In  this case, the individual variation is small enough to be ignored. This can be related to the idea of ‘quantified self’ that transform every individuals into profile data.

While in more serious cases like public health, the prediction based non big data should be taken more carefully. For example, the failure of google flu trend(GFT) brings the reflection to the prediction algorithm. Designed to provide real-time monitor of the flu cases, the GFT draws the conclusion based on the search on words related to the flu. But GFT tends to overestimate the prevalence of flu by more than 50%. According to Time magazine, the “3-week-old data on cases from the CDC” can be used to draw a more accurate result than Google’s sophisticated big data algorithm. The failure in prediction should not be attributed only to the big data, and it seems to be more about an issue of the algorithm design. But this case does brings more reflection on the concept big data. The statistical method usually makes the prediction based on small samples, while the big data and algorithm draw the conclusion based on a big volume of data but sometimes it also brings noise that leads to inaccuracy. A good combination, in general, should be using data mining for data volume, and also adopting statical methods for making precise decision. But this is not that easy.

“What statistics can't tell us about ourselves” used the example of P value to demonstrate that the failure to handle uncertainty is exacerbated in the big data era. Setting threshold is like setting a fixed standard in algorithm, when the volume of data is big enough to raise fair amount of exceptions, this is not enough. Last but not least, going back to the comparison about stats and algorithm, it’s also a question about the purpose. The core to understand a phenomenon or problem is to find causality, but making prediction sometimes don’t have that strict requirements, for example, knowing the correlation could be enough to anticipate what will happen. Statistics perform better in giving rigid explanation, while big data and algorithm perform better in making prediction and validate the data.

### Reference:

- [What We Can Learn From the Epic Failure of Google Flu Trends](https://www.wired.com/2015/10/can-learn-epic-failure-google-flu-trends/)
- [Google's Flu Project Shows the Failings of Big Data](https://time.com/23782/google-flu-trends-big-data-problems/)
- [What Statistics Can and Can’t Tell Us About Ourselves](https://www.newyorker.com/magazine/2019/09/09/what-statistics-can-and-cant-tell-us-about-ourselves)
